{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab942da-1fcb-4ab0-86d2-b850ee9f027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def build_emotion_model(input_shape=(96, 96, 1), num_classes=7):\n",
    "    \"\"\"Build optimized emotion recognition model\"\"\"\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Convert grayscale to 3 channels\n",
    "    x = Concatenate()([input_layer, input_layer, input_layer])\n",
    "    \n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(*input_shape[:2], 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        alpha=0.75\n",
    "    )\n",
    "    \n",
    "    # Freeze initial layers, fine-tune later ones\n",
    "    for layer in base_model.layers[:120]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[120:]:\n",
    "        layer.trainable = True\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            layer.kernel_regularizer = l2(0.0001)\n",
    "    \n",
    "    x = base_model(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "def create_data_generators(train_path, val_path, img_size=(96, 96), batch_size=64):\n",
    "    \"\"\"Create optimized data generators with augmentation\"\"\"\n",
    "    if not os.path.exists(train_path):\n",
    "        raise FileNotFoundError(f\"Training directory not found: {train_path}\")\n",
    "    if not os.path.exists(val_path):\n",
    "        raise FileNotFoundError(f\"Validation directory not found: {val_path}\")\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=25,\n",
    "        zoom_range=0.2,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        shear_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=[0.85, 1.15],\n",
    "        rescale=1./255,\n",
    "        fill_mode='reflect',\n",
    "        channel_shift_range=20.0\n",
    "    )\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=img_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    val_gen = val_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        target_size=img_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    if len(train_gen.class_indices) == 0:\n",
    "        raise ValueError(\"No training images found. Check your directory structure.\")\n",
    "    if len(val_gen.class_indices) == 0:\n",
    "        raise ValueError(\"No validation images found. Check your directory structure.\")\n",
    "\n",
    "    return train_gen, val_gen\n",
    "\n",
    "def train_model(model, train_gen, val_gen, save_path, epochs=100):\n",
    "    \"\"\"Train model with callbacks\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_path,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history\n",
    "def detect_face(image, target_size=(96, 96)):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    (x, y, w, h) = faces[0]  # Take the first face\n",
    "    face = gray[y:y+h, x:x+w]\n",
    "    face_resized = cv2.resize(face, target_size)\n",
    "    return face_resized\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    TRAIN_PATH = r\"C:\\Users\\mahesh\\Documents\\facial_emotion_detection\\images\\train\"\n",
    "    VAL_PATH = r\"C:\\Users\\mahesh\\Documents\\facial_emotion_detection\\images\\validation\"\n",
    "    MODEL_PATH = r\"C:\\Users\\mahesh\\Desktop\\New folder\\emotion_model_final.keras\"\n",
    "    IMG_SIZE = (96, 96)\n",
    "    \n",
    "    try:\n",
    "        print(\"Building model...\")\n",
    "        model = build_emotion_model(input_shape=(*IMG_SIZE, 1))\n",
    "        \n",
    "        print(\"Creating data generators...\")\n",
    "        train_gen, val_gen = create_data_generators(TRAIN_PATH, VAL_PATH, IMG_SIZE)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        train_model(model, train_gen, val_gen, MODEL_PATH)\n",
    "        \n",
    "        print(\"Training complete. Starting detection...\")\n",
    "        detect_emotions(MODEL_PATH, IMG_SIZE)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7948fe4c-7d72-40df-8f6a-ec431578bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load OpenCV's Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def detect_face(frame, img_size):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    x, y, w, h = faces[0]  # Take the first face detected\n",
    "    face = gray[y:y+h, x:x+w]\n",
    "    face = cv2.resize(face, img_size)\n",
    "    return face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b7a0be-d2e4-4fa0-81b8-06c704bfe834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for web cam facial expresion detection\n",
    "def detect_emotions(MODEL_PATH, IMG_SIZE):\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)  # Live webcam feed\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        face_img = detect_face(frame, IMG_SIZE)\n",
    "        if face_img is not None:\n",
    "            norm_img = face_img.astype(\"float32\") / 255.0\n",
    "            norm_img = np.expand_dims(norm_img, axis=[0, -1])  # shape (1, 96, 96, 1)\n",
    "            preds = model.predict(norm_img, verbose=0)[0]\n",
    "            label = labels[np.argmax(preds)]\n",
    "            confidence = np.max(preds)\n",
    "\n",
    "            cv2.putText(frame, f\"{label}: {confidence:.2f}\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"No face detected\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Emotion Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88363d-9cf9-4998-bbdb-6ef891e31a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected Emotion: neutral (0.66) at [181, 49, 57, 57]\n",
      "✅ Detected Emotion: happy (0.62) at [582, 301, 57, 57]\n",
      "✅ Detected Emotion: angry (0.93) at [585, 58, 63, 63]\n",
      "✅ Detected Emotion: angry (0.84) at [184, 310, 60, 60]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Emotion labels (must match the model's training order)\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# Load the trained emotion detection model\n",
    "def load_emotion_model(model_path):\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Detect all faces in an image using Haar Cascade\n",
    "def detect_faces(image, img_size=(96, 96)):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    face_list = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, img_size)\n",
    "        face_list.append((face, (x, y, w, h)))\n",
    "\n",
    "    return face_list\n",
    "\n",
    "# Main function to detect emotion for multiple faces in one image\n",
    "def detect_emotion_static(image_path, model_path, img_size=(96, 96)):\n",
    "    model = load_emotion_model(model_path)\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(\"❌ Image not found.\")\n",
    "        return\n",
    "\n",
    "    # Detect all faces in the image\n",
    "    faces = detect_faces(img, img_size)\n",
    "\n",
    "    if not faces:\n",
    "        print(\"❌ No faces detected.\")\n",
    "        return\n",
    "\n",
    "    for face, (x, y, w, h) in faces:\n",
    "        # Preprocess the face\n",
    "        face_input = face.astype(\"float32\") / 255.0\n",
    "        face_input = np.expand_dims(face_input, axis=(0, -1))  # Shape: (1, 96, 96, 1)\n",
    "\n",
    "        # Predict emotion\n",
    "        preds = model.predict(face_input, verbose=0)[0]\n",
    "        emotion = emotion_labels[np.argmax(preds)]\n",
    "        confidence = np.max(preds)\n",
    "\n",
    "        # Draw box and label on the image\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(img, f\"{emotion} ({confidence:.2f})\", (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        print(f\"✅ Detected Emotion: {emotion} ({confidence:.2f}) at [{x}, {y}, {w}, {h}]\")\n",
    "\n",
    "    # Show the final image with all detections\n",
    "    cv2.imshow(\"Multiple Face Emotion Detection\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# ✅ Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = r\"C:\\Users\\mahesh\\Desktop\\New folder\\emotion_model_final.keras\"\n",
    "    image_path = r\"C:\\Users\\mahesh\\Downloads\\try2.webp\" # Use image with multiple faces\n",
    "    detect_emotion_static(image_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00d29de2-f481-4209-a301-f9d6a54e0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = r\"C:\\Users\\mahesh\\Desktop\\New folder\\emotion_model_final.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3eebc-232a-4e04-a13d-ca64bd4cd80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For real-time detection\n",
    "detect_emotions(MODEL_PATH, (96, 96))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (emotion_env)",
   "language": "python",
   "name": "emotion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
